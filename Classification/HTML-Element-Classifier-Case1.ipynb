{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes,svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.dropna of                    websites_link  \\\n",
       "0        https://www.amazon.com/   \n",
       "1          https://www.ebay.com/   \n",
       "2          https://shop.bbc.com/   \n",
       "3    https://www.aliexpress.com/   \n",
       "4      https://www.amazon.co.uk/   \n",
       "..                           ...   \n",
       "271                    kohls.com   \n",
       "272                 digikala.com   \n",
       "273                      otto.de   \n",
       "274                cdiscount.com   \n",
       "275             ticketmaster.com   \n",
       "\n",
       "                                           HTMLElement     Category  \n",
       "0    <input id=\"twotabsearchtextbox\" value=\"\" name=...       Search  \n",
       "1    <input class=\"gh-tb ui-autocomplete-input\" ari...       Search  \n",
       "2    <input type=\"search\" id=\"bc-sf-search-box-0\" c...       Search  \n",
       "3    <input type=\"text\" placeholder=\"iphone 13 pro ...       Search  \n",
       "4    <input id=\"twotabsearchtextbox\" value=\"\" name=...       Search  \n",
       "..                                                 ...          ...  \n",
       "271  <input type=\"button\" title=\"Add To Cart\" alt=\"...  Add To Cart  \n",
       "272  <button class=\"relative d-flex ai-center user-...  Add To Cart  \n",
       "273  <button data-qa=\"addToBasket\" class=\" prd_orde...  Add To Cart  \n",
       "274  <input class=\"btGreen btS jsValidForm\" onclick...  Add To Cart  \n",
       "275  <button class=\"indexstyles__StyledButton-sc-83...  Add To Cart  \n",
       "\n",
       "[276 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set Random seed\n",
    "np.random.seed(500)\n",
    "\n",
    "# Add the Data using pandas\n",
    "df = pd.read_csv(r\"C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/HTML-elements.csv\",encoding = \"latin-1\")\n",
    "\n",
    "df = df[df.Category != 'Check Out']\n",
    "df.dropna\n",
    "#Corpu.to_excel(r\"C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/AddToCartvsCheckout.xlsx\", encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus = pd.read_excel(r\"C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/AddToCartvsCheckout.xlsx\")\n",
    "#Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['input', 'twotabsearchtextbox', 'auto', 'sear...\n",
      "1    ['input', 'list', 'false', 'search', 'anything...\n",
      "2           ['input', 'search', 'label', 'q', 'input']\n",
      "3    ['input', 'text', 'iphone', 'pro', 'max', 'cas...\n",
      "4    ['input', 'twotabsearchtextbox', 'auto', 'sear...\n",
      "Name: processed_text, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step - 1: Data Pre-processing - This will help in getting better results through the classification algorithms\n",
    "df = df.astype(str)\n",
    "# Step - 1a : Remove blank rows if any.\n",
    "df['HTMLElement'].dropna(inplace=True)\n",
    "\n",
    "# Step - 1b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "df['HTMLElement'] = [entry.lower() for entry in df['HTMLElement']]\n",
    "\n",
    "# Step - 1c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "df['HTMLElement']= [word_tokenize(entry) for entry in df['HTMLElement']]\n",
    "\n",
    "# Step - 1d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for index,entry in enumerate(df['HTMLElement']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    processed_text = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            processed_text.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    df.loc[index,'processed_text'] = str(processed_text)\n",
    "\n",
    "print(df['processed_text'].head())\n",
    "\n",
    "# Step - 2: Split the model into Train and Test Data set\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df['processed_text'],df['Category'],test_size=0.3)\n",
    "\n",
    "# Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y.astype(str))\n",
    "Test_Y = Encoder.fit_transform(Test_Y.astype(str))\n",
    "\n",
    "Test_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Search\n",
      "1    Search\n",
      "2    Search\n",
      "3    Search\n",
      "4    Search\n",
      "Name: Category, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['Category'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:670: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_folds = np.zeros(n_samples, dtype=np.int)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:442: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:442: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\kelse\\Anaconda3\\new\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  98.18181818181819\n",
      "Precision: 0.9629629629629629\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "X, y = df['processed_text'], df['Category'].astype(str)\n",
    "#data_count, batch_count = X.shape\n",
    "#X=np.reshape(X, (data_count*batch_count))\n",
    "#y = np.reshape(y , (data_count*batch_count, -1))\n",
    "#metrics = []\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    \n",
    "    vect = CountVectorizer(ngram_range=(1,2), max_features=1000 , stop_words=\"english\")\n",
    "    X_train_bow = vect.fit_transform(X_train)\n",
    "    X_test_bow = vect.transform(X_test)\n",
    "    #Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "    #Tfidf_vect.fit(df['text_final'])\n",
    "    #Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "    #Test_X_Tfidf = Tfidf_vect.transform(X_test)\n",
    "    #nb = naive_bayes.MultinomialNB()\n",
    "    #nb.fit(X_train_bow, y_train)\n",
    "    SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train_bow,y_train)\n",
    "    predictions_SVM = SVM.predict(X_test_bow)\n",
    "    # save the model to disk\n",
    "    pickle.dump(SVM, open('C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/finalized_svm_model.pickle', 'wb'))\n",
    "    pickle.dump(vect, open(\"C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/vector.pickel\", \"wb\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(y_test,predictions_SVM)*100)\n",
    "print(\"Precision:\",metrics.precision_score(y_test, predictions_SVM,average=\"binary\",pos_label=\"Add To Cart\"))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, predictions_SVM, average=\"binary\",pos_label=\"Add To Cart\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['processed_text'], df['Category'].astype(str)\n",
    "#data_count, batch_count = X.shape\n",
    "#X=np.reshape(X, (data_count*batch_count))\n",
    "#y = np.reshape(y , (data_count*batch_count, -1))\n",
    "#metrics = []\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    \n",
    "    #using BagOfWords\n",
    "    vect = CountVectorizer(ngram_range=(1,2), max_features=1000 , stop_words=\"english\")\n",
    "    X_train_bow = vect.fit_transform(X_train)\n",
    "    X_test_bow = vect.transform(X_test)\n",
    "    \n",
    "    #Using TF-IDF\n",
    "    #vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    #vectorizer.fit(Corpus['text_final'])\n",
    "    #X_train_tfidf = vectorizer.transform(X_train)\n",
    "    #X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    nb = naive_bayes.MultinomialNB()\n",
    "    nb.fit(X_train_bow, y_train)\n",
    "    predictions_NB = nb.predict(X_test_bow)\n",
    "    \n",
    "\n",
    "#metrics = np.array(metrics)\n",
    "#print('Mean accuracy: ', np.mean(metrics, axis=0))\n",
    "#print('Std for accuracy: ', np.std(metrics, axis=0))\n",
    "\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(y_test,predictions_NB)*100)\n",
    "print(\"Precision:\",metrics.precision_score(y_test, predictions_NB,average=\"binary\",pos_label=\"Add To Cart\"))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, predictions_NB,average=\"binary\",pos_label=\"Add To Cart\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Generate the confusion matrix\n",
    "cf_matrix = confusion_matrix(Test_Y, predictions_SVM)\n",
    "\n",
    "print(cf_matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()\n",
    "\n",
    "print (classification_report(y_test, predictions_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    element = [entry.lower() for entry in text]\n",
    "    \n",
    "    element= [word_tokenize(entry) for entry in text] #tokenization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    final_words = []\n",
    "    for word in element:\n",
    "        if word not in stopwords.words('english'):\n",
    "            final_words.append(word)\n",
    "            \n",
    "    text_processed =[ ' '.join([lemmatizer.lemmatize(word) for word_list in element for word in word_list])]\n",
    "    \n",
    "    \n",
    "    return text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Search = ['<input class=\"quer\" type=\"text\" value=\"\" style=\"border: solid #b9b9b9 2px ; height: 3em; width: 78%; border-radius:10px; padding-left: 8%; font-size: 1.5em; font-weight: 600; outline: none; margin-top: 2%\" placeholder=\"Search\">']\n",
    "test_AddToCart = ['<input type=\"button\" id=\"add-to-cart-button-38224\" value=\"Add to cart\" class=\"button-1 add-to-cart-button nopAjaxCartProductVariantAddToCartButton\" data-productid=\"38224\">']\n",
    "#test_Checkout = ['<button type=\"button\" class=\"btnTemplateCheckOut\" data-toggle=\"modal\" data-target=\"#exampleModalLong\">Secure CheckOut</button>']\n",
    "text = preprocess_text(test_Search)\n",
    "test_input = vect.transform(text)\n",
    "test_input.shape\n",
    "print(test_Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the model \n",
    "res= SVM.predict(test_input)[0]\n",
    "print(res,text)\n",
    "if res==1:\n",
    "    print(\"Search\")\n",
    "elif res==0:\n",
    "    print(\"AddToCart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(test_Search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uvicorn\n",
    "from os.path import dirname, join, realpath\n",
    "import joblib\n",
    "from typing import List\n",
    "from typing import Optional\n",
    "from fastapi import FastAPI, Query\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"HTML Element Classifier Model API\",\n",
    "    description=\"API for ML model\",\n",
    "    version=\"0.1\",\n",
    ")\n",
    "\n",
    "# load model\n",
    "#with open(\n",
    "#    join(dirname(realpath('C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/')), \"finalized_svm_model.pkl\"), \"rb\"\n",
    "#) as f:\n",
    " #   model = joblib.load(f)\n",
    "\n",
    "model=joblib.load('C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/finalized_svm_model.pickle')\n",
    "    \n",
    "def preprocess_text(text):\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    element = [entry.lower() for entry in text]\n",
    "    \n",
    "    #Step - 1c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    element= [word_tokenize(entry) for entry in text]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    final_words = []\n",
    "    for word in element:\n",
    "        if word not in stopwords.words('english'):\n",
    "            final_words.append(word)\n",
    "            \n",
    "    text_processed =[ ' '.join([lemmatizer.lemmatize(word) for word_list in element for word in word_list])]\n",
    "    \n",
    "    return text_processed\n",
    "\n",
    "\n",
    "@app.get(\"/predict-element\")\n",
    "def predict(element: Optional[List[str]] = Query([\"<input></input>\"])):\n",
    "    \n",
    "    ##Pre-process text\n",
    "    processed_text = preprocess_text(element)\n",
    "   \n",
    "   # vect = CountVectorizer(decode_error=\" Replace \", vocabulary=p.load(open(\"C:/Users/kelse/OneDrive/Documents/UOM-Imp Info/FYP/vector.pickel\", \"rb\")))\n",
    "    vect = CountVectorizer(ngram_range=(1,2), max_features=1000 , stop_words=\"english\")\n",
    "    test = vect.transform(processed_text)\n",
    "    \n",
    "   \n",
    "    prediction = model.predict(test)[0]\n",
    "    output = int(prediction[0])\n",
    "   \n",
    "    \n",
    "    # output dictionary\n",
    "    pred = {0: \"AddToCart\", 1: \"Search\"}\n",
    "    \n",
    "    # show results\n",
    "    result = {\"prediction\": pred[prediction]}\n",
    "\n",
    "    return result\n",
    "\n",
    "@app.exception_handler(ValueError)\n",
    "async def value_error_exception_handler(request: Request, exc: ValueError):\n",
    "    return JSONResponse(\n",
    "        status_code=400,\n",
    "        content={\"message\": str(exc)},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18724]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:10479 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:10479 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:10484 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:10484 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:10484 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:10506 - \"GET /predict-element?element=%3C%20input%20class%3D%20%27%27%20quer%20%27%27%20type%3D%20%27%27%20text%20%27%27%20value%3D%20%27%27%20%27%27%20style%3D%20%27%27%20border%20%3A%20solid%20%23%20b9b9b9%202px%20%3B%20height%20%3A%203em%20%3B%20width%20%3A%2078%20%25%20%3B%20border-radius%3A10px%20%3B%20padding-left%20%3A%208%20%25%20%3B%20font-size%20%3A%201.5em%20%3B%20font-weight%20%3A%20600%20%3B%20outline%20%3A%20none%20%3B%20margin-top%20%3A%202%20%25%20%27%27%20placeholder%3D%20%27%27%20Search%20%27%27%20%3E HTTP/1.1\" 400 Bad Request\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "\n",
    "ngrok_tunnel\n",
    "\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
